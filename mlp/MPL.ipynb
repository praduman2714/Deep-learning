{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d76a0bc2",
            "metadata": {},
            "source": [
                "# Multi-Layer Perceptron (MLP) from Scratch\n",
                "\n",
                "In this notebook, we will build a simple Multi-Layer Perceptron (MLP) to solve the XOR problem. We will do this in two ways:\n",
                "1.  **From Scratch (using NumPy)**: To understand the underlying mathematics (forward pass, backpropagation, gradient descent).\n",
                "2.  **Using TensorFlow/Keras**: To see how it's done in a modern deep learning framework."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d53fb6c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e8d87ec2",
            "metadata": {},
            "source": [
                "## 1. The XOR Dataset\n",
                "\n",
                "The XOR function is a classic problem that a single linear layer (perceptron) cannot solve. It requires a hidden layer to capture the non-linearity.\n",
                "\n",
                "| Input 1 | Input 2 | Output |\n",
                "| :---: | :---: | :---: |\n",
                "| 0 | 0 | 0 |\n",
                "| 0 | 1 | 1 |\n",
                "| 1 | 0 | 1 |\n",
                "| 1 | 1 | 0 |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28d6ced5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# XOR dataset\n",
                "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y = np.array([[0], [1], [1], [0]])\n",
                "\n",
                "print(\"X shape:\", X.shape)\n",
                "print(\"y shape:\", y.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8f06a601",
            "metadata": {},
            "source": [
                "## 2. MLP from Scratch (NumPy)\n",
                "\n",
                "We will build a simple network with:\n",
                "-   **Input Layer**: 2 neurons (for the two inputs)\n",
                "-   **Hidden Layer**: 2 neurons (sufficient for XOR)\n",
                "-   **Output Layer**: 1 neuron (binary classification)\n",
                "-   **Activation**: Sigmoid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8e8eee78",
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(x):\n",
                "    return 1 / (1 + np.exp(-x))\n",
                "\n",
                "def sigmoid_derivative(x):\n",
                "    return x * (1 - x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d7acbd5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialization\n",
                "input_size = 2\n",
                "hidden_size = 2\n",
                "output_size = 1\n",
                "learning_rate = 0.1\n",
                "epochs = 10000\n",
                "\n",
                "# Weights and Biases\n",
                "# W1: weights between input and hidden layer\n",
                "# b1: biases for hidden layer\n",
                "# W2: weights between hidden and output layer\n",
                "# b2: biases for output layer\n",
                "\n",
                "np.random.seed(42)\n",
                "W1 = np.random.uniform(size=(input_size, hidden_size))\n",
                "b1 = np.random.uniform(size=(1, hidden_size))\n",
                "W2 = np.random.uniform(size=(hidden_size, output_size))\n",
                "b2 = np.random.uniform(size=(1, output_size))\n",
                "\n",
                "print(\"W1 shape:\", W1.shape)\n",
                "print(\"W2 shape:\", W2.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d12c689",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training Loop\n",
                "losses = []\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    # --- Forward Pass ---\n",
                "    # Layer 1 (Hidden)\n",
                "    hidden_input = np.dot(X, W1) + b1\n",
                "    hidden_output = sigmoid(hidden_input)\n",
                "    \n",
                "    # Layer 2 (Output)\n",
                "    final_input = np.dot(hidden_output, W2) + b2\n",
                "    final_output = sigmoid(final_input)\n",
                "    \n",
                "    # --- Loss (MSE) ---\n",
                "    error = y - final_output\n",
                "    loss = np.mean(np.square(error))\n",
                "    losses.append(loss)\n",
                "    \n",
                "    # --- Backpropagation ---\n",
                "    # Calculate gradients\n",
                "    # d_loss/d_output * d_output/d_input\n",
                "    d_output = error * sigmoid_derivative(final_output)\n",
                "    \n",
                "    # Error at hidden layer\n",
                "    error_hidden_layer = d_output.dot(W2.T)\n",
                "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_output)\n",
                "    \n",
                "    # --- Update Weights (Gradient Descent) ---\n",
                "    W2 += hidden_output.T.dot(d_output) * learning_rate\n",
                "    b2 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
                "    W1 += X.T.dot(d_hidden_layer) * learning_rate\n",
                "    b1 += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
                "    \n",
                "    if epoch % 1000 == 0:\n",
                "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
                "\n",
                "print(f\"Final Loss: {loss:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ed36e00",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Testing the NumPy Model\n",
                "print(\"Predictions:\")\n",
                "print(final_output)\n",
                "print(\"\\nRounded Predictions:\")\n",
                "print(np.round(final_output))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fbc76522",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(losses)\n",
                "plt.title('Training Loss (NumPy)')\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('MSE Loss')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6d57854d",
            "metadata": {},
            "source": [
                "## 3. MLP using TensorFlow/Keras\n",
                "\n",
                "Now let's do the exact same thing using TensorFlow. Notice how much simpler the code is."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7b48b08",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = tf.keras.Sequential([\n",
                "    tf.keras.layers.Dense(2, input_dim=2, activation='sigmoid'), # Hidden layer\n",
                "    tf.keras.layers.Dense(1, activation='sigmoid')               # Output layer\n",
                "])\n",
                "\n",
                "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
                "              loss='mean_squared_error',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "history = model.fit(X, y, epochs=10000, verbose=0)\n",
                "\n",
                "print(\"Final Loss:\", history.history['loss'][-1])\n",
                "print(\"Final Accuracy:\", history.history['accuracy'][-1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fdca003c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Testing the TF Model\n",
                "print(model.predict(X))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv (3.12.3)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
